{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Article Predictions "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### We made a article recommendation system to predict the articles based on user intrests which is given in the form of prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 702
        },
        "id": "il3k3YyAUKPt",
        "outputId": "e7d0516e-5e00-4655-8d37-197a55739d33"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>item_title</th>\n",
              "      <th>publication_title</th>\n",
              "      <th>item_doi</th>\n",
              "      <th>authors</th>\n",
              "      <th>publication_year</th>\n",
              "      <th>url</th>\n",
              "      <th>keywords</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>correction strong field physics pursued with p...</td>\n",
              "      <td>aapps bulletin</td>\n",
              "      <td>10.1007/s43673-021-00009-0</td>\n",
              "      <td>vishwa bandhu pathakseong ku leeki hong paecal...</td>\n",
              "      <td>2021</td>\n",
              "      <td>http://link.springer.com/article/10.1007/s4367...</td>\n",
              "      <td>AAPPS Bulletin Atomic Molecular Optical and Pl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>time reversal and reciprocity</td>\n",
              "      <td>aapps bulletin</td>\n",
              "      <td>10.1007/s43673-022-00060-5</td>\n",
              "      <td>olivier sigwarthchristian miniatura</td>\n",
              "      <td>2022</td>\n",
              "      <td>http://link.springer.com/article/10.1007/s4367...</td>\n",
              "      <td>AAPPS Bulletin Atomic Molecular Optical and Pl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ultrasound detection using microcavity raman l...</td>\n",
              "      <td>aapps bulletin</td>\n",
              "      <td>10.1007/s43673-022-00068-x</td>\n",
              "      <td>xiao</td>\n",
              "      <td>2022</td>\n",
              "      <td>http://link.springer.com/article/10.1007/s4367...</td>\n",
              "      <td>AAPPS Bulletin Atomic Molecular Optical and Pl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>relativistic density functional theory in nucl...</td>\n",
              "      <td>aapps bulletin</td>\n",
              "      <td>10.1007/s43673-021-00001-8</td>\n",
              "      <td>jie mengpengwei zhao</td>\n",
              "      <td>2021</td>\n",
              "      <td>http://link.springer.com/article/10.1007/s4367...</td>\n",
              "      <td>AAPPS Bulletin Atomic Molecular Optical and Pl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>muon cooling and acceleration</td>\n",
              "      <td>aapps bulletin</td>\n",
              "      <td>10.1007/s43673-022-00035-6</td>\n",
              "      <td>masashi otani</td>\n",
              "      <td>2022</td>\n",
              "      <td>http://link.springer.com/article/10.1007/s4367...</td>\n",
              "      <td>AAPPS Bulletin Atomic Molecular Optical and Pl...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          item_title publication_title  \\\n",
              "0  correction strong field physics pursued with p...    aapps bulletin   \n",
              "1                      time reversal and reciprocity    aapps bulletin   \n",
              "2  ultrasound detection using microcavity raman l...    aapps bulletin   \n",
              "3  relativistic density functional theory in nucl...    aapps bulletin   \n",
              "4                      muon cooling and acceleration    aapps bulletin   \n",
              "\n",
              "                     item_doi  \\\n",
              "0  10.1007/s43673-021-00009-0   \n",
              "1  10.1007/s43673-022-00060-5   \n",
              "2  10.1007/s43673-022-00068-x   \n",
              "3  10.1007/s43673-021-00001-8   \n",
              "4  10.1007/s43673-022-00035-6   \n",
              "\n",
              "                                             authors  publication_year  \\\n",
              "0  vishwa bandhu pathakseong ku leeki hong paecal...              2021   \n",
              "1                olivier sigwarthchristian miniatura              2022   \n",
              "2                                               xiao              2022   \n",
              "3                               jie mengpengwei zhao              2021   \n",
              "4                                      masashi otani              2022   \n",
              "\n",
              "                                                 url  \\\n",
              "0  http://link.springer.com/article/10.1007/s4367...   \n",
              "1  http://link.springer.com/article/10.1007/s4367...   \n",
              "2  http://link.springer.com/article/10.1007/s4367...   \n",
              "3  http://link.springer.com/article/10.1007/s4367...   \n",
              "4  http://link.springer.com/article/10.1007/s4367...   \n",
              "\n",
              "                                            keywords  \n",
              "0  AAPPS Bulletin Atomic Molecular Optical and Pl...  \n",
              "1  AAPPS Bulletin Atomic Molecular Optical and Pl...  \n",
              "2  AAPPS Bulletin Atomic Molecular Optical and Pl...  \n",
              "3  AAPPS Bulletin Atomic Molecular Optical and Pl...  \n",
              "4  AAPPS Bulletin Atomic Molecular Optical and Pl...  "
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "main=pd.read_csv('cleaned_text.csv')\n",
        "main.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(275755, 7)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "main.shape"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3Fl-nftdxdj"
      },
      "source": [
        "### Journal Dataframe"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Journal articles, keywords, authors concating them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "rAqy-9tSd1LQ",
        "outputId": "715359b6-fcc8-4c48-ca43-4837c57ddb94"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'Publication Title'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m journal_art \u001b[39m=\u001b[39m main\u001b[39m.\u001b[39;49mgroupby(\u001b[39m'\u001b[39;49m\u001b[39mPublication Title\u001b[39;49m\u001b[39m'\u001b[39;49m)[\u001b[39m'\u001b[39m\u001b[39mItem Title\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlist\u001b[39m)\u001b[39m.\u001b[39mreset_index(name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mArticles\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m journal_art\u001b[39m.\u001b[39mset_index([\u001b[39m'\u001b[39m\u001b[39mPublication Title\u001b[39m\u001b[39m'\u001b[39m],inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m journal_auth \u001b[39m=\u001b[39m main\u001b[39m.\u001b[39mgroupby(\u001b[39m'\u001b[39m\u001b[39mPublication Title\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m'\u001b[39m\u001b[39mAuthors\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlist\u001b[39m)\u001b[39m.\u001b[39mreset_index(name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mAuthors\u001b[39m\u001b[39m'\u001b[39m)\n",
            "File \u001b[1;32mc:\\Python 3.11.0\\Lib\\site-packages\\pandas\\core\\frame.py:8252\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[1;34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[0;32m   8249\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou have to supply one of \u001b[39m\u001b[39m'\u001b[39m\u001b[39mby\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlevel\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   8250\u001b[0m axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_axis_number(axis)\n\u001b[1;32m-> 8252\u001b[0m \u001b[39mreturn\u001b[39;00m DataFrameGroupBy(\n\u001b[0;32m   8253\u001b[0m     obj\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   8254\u001b[0m     keys\u001b[39m=\u001b[39;49mby,\n\u001b[0;32m   8255\u001b[0m     axis\u001b[39m=\u001b[39;49maxis,\n\u001b[0;32m   8256\u001b[0m     level\u001b[39m=\u001b[39;49mlevel,\n\u001b[0;32m   8257\u001b[0m     as_index\u001b[39m=\u001b[39;49mas_index,\n\u001b[0;32m   8258\u001b[0m     sort\u001b[39m=\u001b[39;49msort,\n\u001b[0;32m   8259\u001b[0m     group_keys\u001b[39m=\u001b[39;49mgroup_keys,\n\u001b[0;32m   8260\u001b[0m     observed\u001b[39m=\u001b[39;49mobserved,\n\u001b[0;32m   8261\u001b[0m     dropna\u001b[39m=\u001b[39;49mdropna,\n\u001b[0;32m   8262\u001b[0m )\n",
            "File \u001b[1;32mc:\\Python 3.11.0\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:931\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[1;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropna \u001b[39m=\u001b[39m dropna\n\u001b[0;32m    930\u001b[0m \u001b[39mif\u001b[39;00m grouper \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 931\u001b[0m     grouper, exclusions, obj \u001b[39m=\u001b[39m get_grouper(\n\u001b[0;32m    932\u001b[0m         obj,\n\u001b[0;32m    933\u001b[0m         keys,\n\u001b[0;32m    934\u001b[0m         axis\u001b[39m=\u001b[39;49maxis,\n\u001b[0;32m    935\u001b[0m         level\u001b[39m=\u001b[39;49mlevel,\n\u001b[0;32m    936\u001b[0m         sort\u001b[39m=\u001b[39;49msort,\n\u001b[0;32m    937\u001b[0m         observed\u001b[39m=\u001b[39;49mobserved,\n\u001b[0;32m    938\u001b[0m         dropna\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropna,\n\u001b[0;32m    939\u001b[0m     )\n\u001b[0;32m    941\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj \u001b[39m=\u001b[39m obj\n\u001b[0;32m    942\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39m_get_axis_number(axis)\n",
            "File \u001b[1;32mc:\\Python 3.11.0\\Lib\\site-packages\\pandas\\core\\groupby\\grouper.py:985\u001b[0m, in \u001b[0;36mget_grouper\u001b[1;34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[0m\n\u001b[0;32m    983\u001b[0m         in_axis, level, gpr \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, gpr, \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    984\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 985\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(gpr)\n\u001b[0;32m    986\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(gpr, Grouper) \u001b[39mand\u001b[39;00m gpr\u001b[39m.\u001b[39mkey \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    987\u001b[0m     \u001b[39m# Add key to exclusions\u001b[39;00m\n\u001b[0;32m    988\u001b[0m     exclusions\u001b[39m.\u001b[39madd(gpr\u001b[39m.\u001b[39mkey)\n",
            "\u001b[1;31mKeyError\u001b[0m: 'Publication Title'"
          ]
        }
      ],
      "source": [
        "journal_art = main.groupby('Publication Title')['Item Title'].apply(list).reset_index(name='Articles')\n",
        "journal_art.set_index(['Publication Title'],inplace=True)\n",
        "journal_auth = main.groupby('Publication Title')['Authors'].apply(list).reset_index(name='Authors')\n",
        "journal_auth.set_index(['Publication Title'],inplace=True)\n",
        "journal_key= main.drop_duplicates(subset=[\"Publication Title\", \"Keywords\"], keep='first')\n",
        "journal_key=journal_key.drop(['Item Title','Authors','Publication Year','URL'],axis=1)\n",
        "journal_key.set_index(['Publication Title'],inplace=True)\n",
        "journal_main = pd.concat([journal_key, journal_art,journal_auth], axis=1, join='inner')\n",
        "journal_main=journal_main.reset_index()\n",
        "journal_main.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Changing into paragraphs from lists"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### get paragraph:\n",
        "1. combines joins the list of the data into a paragraph\n",
        "##### get cleaned_text:\n",
        "1. check if the instance is string\n",
        "2. re.match(r'^[a-zA-Z]+$', word) checks if the word consists only of alphabetic characters\n",
        "3. word not in stop_words checks if the word is not in a list of stop words.\n",
        "4. len(word) > 1 checks if the word has a length greater than 1.\n",
        "5. word[1] != '.' checks if the second character of the word is not a dot (.) character.\n",
        "##### combine text:\n",
        "1. combines all the indices required in the list of indices into a paragraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\vukya\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\vukya\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\vukya\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "def get_paragraph(row,index):\n",
        "  ans=''\n",
        "  for x in row[index]:\n",
        "    ans=ans+'  '+x.lower()\n",
        "  return ans\n",
        "def get_clean_text(row,index):\n",
        "    if not isinstance(row[index], str):\n",
        "        return '' \n",
        "    clean_text=''\n",
        "    words = re.findall(r'\\b\\w+\\b', row[index].lower())\n",
        "    words = row[index].lower().split()\n",
        "    for word in words:\n",
        "        if( re.match(r'^[a-zA-Z]+$', word) and  word not in stop_words and len(word) >1 and word[1] != '.'):\n",
        "            clean_text=clean_text+' '+word\n",
        "    return clean_text\n",
        "def combine(row,indices):\n",
        "    ans=''\n",
        "    for i in indices:\n",
        "      ans=ans+' '+row[i]\n",
        "    return ans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Publication Title</th>\n",
              "      <th>Keywords</th>\n",
              "      <th>Articles</th>\n",
              "      <th>Authors</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AAPPS Bulletin</td>\n",
              "      <td>aapps bulletin atomic molecular optical plasm...</td>\n",
              "      <td>correction strong field physics pursued petaw...</td>\n",
              "      <td>vishwa bandhu pathakseong ku leeki hong paeca...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>AAPS PharmSciTech</td>\n",
              "      <td>aaps pharmscitech pharmacologytoxicology biot...</td>\n",
              "      <td>environmental monitoring closed robotic workc...</td>\n",
              "      <td>joseph mccallnonita barnardkevin gadientchand...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>AI &amp; SOCIETY</td>\n",
              "      <td></td>\n",
              "      <td>correction dismantling chinese room linguisti...</td>\n",
              "      <td>lawrence lengbeyer clare foster francesca fof...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>AI and Ethics</td>\n",
              "      <td></td>\n",
              "      <td>publisher convergence source control actual a...</td>\n",
              "      <td>haleh asgarinia jakob floridi robert hannaemr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>AI in Civil Engineering</td>\n",
              "      <td></td>\n",
              "      <td>ai civil engineering fusion thermal rgb image...</td>\n",
              "      <td>xianzhong zhao quincy alexandervedhus hoskere...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Publication Title                                           Keywords  \\\n",
              "0           AAPPS Bulletin   aapps bulletin atomic molecular optical plasm...   \n",
              "1        AAPS PharmSciTech   aaps pharmscitech pharmacologytoxicology biot...   \n",
              "2             AI & SOCIETY                                                      \n",
              "3            AI and Ethics                                                      \n",
              "4  AI in Civil Engineering                                                      \n",
              "\n",
              "                                            Articles  \\\n",
              "0   correction strong field physics pursued petaw...   \n",
              "1   environmental monitoring closed robotic workc...   \n",
              "2   correction dismantling chinese room linguisti...   \n",
              "3   publisher convergence source control actual a...   \n",
              "4   ai civil engineering fusion thermal rgb image...   \n",
              "\n",
              "                                             Authors  \n",
              "0   vishwa bandhu pathakseong ku leeki hong paeca...  \n",
              "1   joseph mccallnonita barnardkevin gadientchand...  \n",
              "2   lawrence lengbeyer clare foster francesca fof...  \n",
              "3   haleh asgarinia jakob floridi robert hannaemr...  \n",
              "4   xianzhong zhao quincy alexandervedhus hoskere...  "
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "journal_main['Articles']=journal_main.apply(get_paragraph,index='Articles',axis=1)\n",
        "journal_main['Articles']=journal_main.apply(get_clean_text,index='Articles',axis=1)\n",
        "journal_main['Authors']=journal_main.apply(get_paragraph,index='Authors',axis=1)\n",
        "journal_main['Authors']=journal_main.apply(get_clean_text,index='Authors',axis=1)\n",
        "journal_main['Keywords']=journal_main.apply(get_clean_text,index=1,axis=1)\n",
        "journal_main.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature Extraction taking nouns and adjectives as features"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Preprocessing the text"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is possible to predict journals using only nouns and adjectives as features. By focusing on nouns and adjectives, we can capture key information and characteristics of the articles that may be relevant for predicting the corresponding journals."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "journal_main['Tokenized'] = journal_main['Articles'].apply(word_tokenize)\n",
        "journal_main['Tagged'] = journal_main['Tokenized'].apply(pos_tag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Publication Title</th>\n",
              "      <th>Tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AAPPS Bulletin</td>\n",
              "      <td>aapps bulletin atomic molecular optical plasm...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>AAPS PharmSciTech</td>\n",
              "      <td>aaps pharmscitech pharmacologytoxicology biot...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>AI &amp; SOCIETY</td>\n",
              "      <td>correction chinese room linguistic framework ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>AI and Ethics</td>\n",
              "      <td>publisher convergence source control actual a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>AI in Civil Engineering</td>\n",
              "      <td>civil engineering fusion thermal rgb images d...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Publication Title                                               Tags\n",
              "0           AAPPS Bulletin   aapps bulletin atomic molecular optical plasm...\n",
              "1        AAPS PharmSciTech   aaps pharmscitech pharmacologytoxicology biot...\n",
              "2             AI & SOCIETY   correction chinese room linguistic framework ...\n",
              "3            AI and Ethics   publisher convergence source control actual a...\n",
              "4  AI in Civil Engineering   civil engineering fusion thermal rgb images d..."
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "journal_main['Tags'] = journal_main['Tagged'].apply(lambda x: [word for word, tag in x if tag.startswith('NN') or tag.startswith('JJ')])\n",
        "journal_main['Tags'] = journal_main['Tags'].apply(lambda x: [word for word in x if word.lower() not in stop_words])\n",
        "journal_main['Tags']=journal_main.apply(get_paragraph,index='Tags',axis=1)\n",
        "journal_main=journal_main.drop(['Articles','Tokenized','Tagged'],axis=1)\n",
        "journal_main['Tags']=journal_main.apply(combine,indices=['Keywords','Tags','Authors'],axis=1)\n",
        "journal_main['Tags']=journal_main.apply(get_clean_text,index='Tags',axis=1)\n",
        "journal_main=journal_main.drop(['Keywords','Authors'],axis=1)\n",
        "journal_main.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TFIDF vector for journal prediction "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ZkbEdmV4dsOz"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "vectorizer = TfidfVectorizer(decode_error='ignore',strip_accents='ascii')\n",
        "journal_tfidf_matrix = vectorizer.fit_transform(journal_main['Tags'])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "we predict based on cosine similarities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "journal_threshold=4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDF4ynyZftUU",
        "outputId": "43afa80d-eb18-4b69-f311-31e4fb25cadb"
      },
      "outputs": [],
      "source": [
        "def get_journal_index(user_input):\n",
        "    user_tfidf = vectorizer.transform([user_input])\n",
        "    cosine_similarities = cosine_similarity(user_tfidf, journal_tfidf_matrix).flatten()\n",
        "    indices = cosine_similarities.argsort()[::-1]\n",
        "    top_recommendations = [i for i in indices if cosine_similarities[i] > 0][:min(journal_threshold, len(indices))]\n",
        "    return top_recommendations"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Preparing individual article dataframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "mOvtQlE2ion5"
      },
      "outputs": [],
      "source": [
        "def get_article_df(row):\n",
        "    article = main.loc[main['Publication Title'] == journal_main['Publication Title'][row.name]].copy()\n",
        "    article['Item Title']=article.apply(get_clean_text,index='Item Title',axis=1)\n",
        "    article['Authors']=article.apply(get_clean_text,index='Authors',axis=1)\n",
        "    article['Tokenized'] = article['Item Title'].apply(word_tokenize)\n",
        "    article['Tagged'] = article['Tokenized'].apply(pos_tag)\n",
        "    article['Tags'] = article['Tagged'].apply(lambda x: [word for word, tag in x if tag.startswith('NN') or tag.startswith('JJ')and word.lower() not in stop_words])\n",
        "    article['Tags']=article.apply(get_paragraph,index='Tags',axis=1)\n",
        "    article['Tags']=article.apply(lambda x : x['Tags']+' '+x['Authors']+' '+str(x['Publication Year']),axis=1)\n",
        "    article=article.drop(['Keywords','Publication Title','Tokenized','Tagged','Authors','Publication Year'],axis=1)\n",
        "    article.reset_index(inplace=True)\n",
        "    article.set_index('index', inplace=True)\n",
        "    return article\n",
        "\n",
        "journal_main['article_df']=journal_main.apply(get_article_df,axis=1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Generating tfidf matrices for the journals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_vectorizer(row):\n",
        "    vectorizer = TfidfVectorizer(decode_error='ignore',strip_accents='ascii')\n",
        "    return vectorizer\n",
        "def get_tfidf_matrix(row):\n",
        "    tfidf_matrix = row['article_vectorizer'].fit_transform(row['article_df']['Tags'])\n",
        "    return tfidf_matrix\n",
        "journal_main['article_vectorizer']=journal_main.apply(get_vectorizer,axis=1)\n",
        "journal_main['article_matrix']=journal_main.apply(get_tfidf_matrix,axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "article_threshold=10"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Article recommendation using user input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_article_recommendations(user_input):\n",
        "    recommended_journals=get_journal_index(user_input)\n",
        "    l=[]\n",
        "    for journal_id in recommended_journals:\n",
        "        user_tfidf=journal_main['article_vectorizer'][journal_id].transform([user_input])\n",
        "        cosine_similarities = cosine_similarity(user_tfidf, journal_main['article_matrix'][journal_id]).flatten()\n",
        "        indices = cosine_similarities.argsort()[::-1]\n",
        "        top_recommendation_articles = [(cosine_similarities[i],i,journal_id) for i in indices if cosine_similarities[i] > 0][:min(article_threshold, len(indices))]\n",
        "        l=l+top_recommendation_articles\n",
        "    l.sort(reverse=True)\n",
        "    return l\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(' language test activism', 'http://link.springer.com/article/10.1007/s10993-022-09614-7'), (' language advocacy times securitization network languagerights', 'http://link.springer.com/article/10.1007/s10993-022-09617-4'), (' attribution autonomy role robotic language acquisition', 'http://link.springer.com/article/10.1007/s00146-020-01114-8'), (' auditing large language approach', 'http://link.springer.com/article/10.1007/s43681-023-00289-2'), (' officiality strategic ambiguity language exploring migrant experiences andorra luxembourg', 'http://link.springer.com/article/10.1007/s10993-021-09602-3'), (' bias comparison framework abusive language datasets', 'http://link.springer.com/article/10.1007/s43681-021-00081-0'), (' ai management beyond exploring ai organizational context', 'http://link.springer.com/article/10.1007/s00146-021-01249-2'), (' automated occupation coding hierarchical approach classification language models', 'http://link.springer.com/article/10.1007/s44163-023-00050-y'), (' natural language processing analysis applied opinions using distilbert model sentiment categorization', 'http://link.springer.com/article/10.1007/s00146-022-01594-w'), (' identity ai', 'http://link.springer.com/article/10.1007/s44163-022-00038-0'), (' evolution language ideological debates english french multilingual humanitarian organisation', 'http://link.springer.com/article/10.1007/s10993-021-09586-0'), (' reflections human role ai policy national ai strategies view', 'http://link.springer.com/article/10.1007/s44163-022-00019-3'), (' analysis news sentiments using natural language processing deep learning', 'http://link.springer.com/article/10.1007/s00146-020-01111-x'), (' ai doctor see assessing framing ai news coverage', 'http://link.springer.com/article/10.1007/s00146-021-01145-9'), (' subnational ai shaping ai governance system', 'http://link.springer.com/article/10.1007/s00146-022-01561-5'), (' explainable ai lacks regulative ai human equally opaque', 'http://link.springer.com/article/10.1007/s43681-022-00217-w'), (' making ai ai futures frames german political media discourses', 'http://link.springer.com/article/10.1007/s00146-021-01161-9'), (' ai social theory', 'http://link.springer.com/article/10.1007/s00146-021-01222-z'), (' ai framework measuring embodied carbon ai systems', 'http://link.springer.com/article/10.1007/s43681-021-00071-2'), (' perceptions attitudes qatar university students regarding utility arabic english communication education qatar', 'http://link.springer.com/article/10.1007/s10993-021-09590-4'), (' identity ai', 'http://link.springer.com/article/10.1007/s44163-023-00054-8'), (' perspectives ai adoption role italian ai strategy', 'http://link.springer.com/article/10.1007/s44163-022-00025-5'), (' tech industry hijacking ai ethics research agenda reclaim', 'http://link.springer.com/article/10.1007/s44163-022-00043-3'), (' uselessness ai ethics', 'http://link.springer.com/article/10.1007/s43681-022-00209-w'), (' openness privacy reflecting role ai development', 'http://link.springer.com/article/10.1007/s00146-021-01361-3'), (' vision lack alignment ai strategies energy regulations dutch electricity sector', 'http://link.springer.com/article/10.1007/s44163-022-00040-6'), (' ai ethics review three recent publications', 'http://link.springer.com/article/10.1007/s00146-020-01087-8'), (' putting ai ethics tools fit', 'http://link.springer.com/article/10.1007/s43681-021-00084-x'), (' beyond implementing ethical ai', 'http://link.springer.com/article/10.1007/s43681-020-00011-6'), (' rawlsian ai fairness loopholes', 'http://link.springer.com/article/10.1007/s43681-022-00226-9'), (' language planning multilingual minoritized language perspective', 'http://link.springer.com/article/10.1007/s10993-015-9397-4'), (' glitters trustworthy ethical ai principles', 'http://link.springer.com/article/10.1007/s43681-022-00232-x'), (' language anishinaabemowin language planning using', 'http://link.springer.com/article/10.1007/s10993-023-09656-5'), (' ai ethics systemic risks finance', 'http://link.springer.com/article/10.1007/s43681-021-00129-1'), (' public sector ai transparency uk government seeks lead example', 'http://link.springer.com/article/10.1007/s44163-022-00018-4'), (' ai impacts supply chain performance manufacturing use case study', 'http://link.springer.com/article/10.1007/s44163-023-00061-9'), (' charting ai conceptual sources spatial implications urban artificial intelligence', 'http://link.springer.com/article/10.1007/s44163-023-00060-w'), (' normative language policy minority language rethinking case regional languages france', 'http://link.springer.com/article/10.1007/s10993-016-9411-5'), (' linguistic still valid construct relation language policy irish sign language', 'http://link.springer.com/article/10.1007/s10993-017-9446-2'), (' new speakers new old investigation gap language practices language policy', 'http://link.springer.com/article/10.1007/s10993-018-9503-5')]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def get_links(user_input):\n",
        "    l=[]\n",
        "    recommendation_list=get_article_recommendations(user_input) #You will get article id and journal id from here\n",
        "    for article in recommendation_list:\n",
        "        cosine_similarity,article_id,journal_id=article\n",
        "        l.append((journal_main['article_df'][journal_id].iloc[article_id,0],journal_main['article_df'][journal_id].iloc[article_id,1],article_id,journal_id))\n",
        "        # print(name,url)\n",
        "    return l\n",
        "user_input = \"AI is my favourite language in 2022\"\n",
        "l=get_links(user_input)\n",
        "print(l)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
