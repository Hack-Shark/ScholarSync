{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Article Predictions "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### We made a article recommendation system to predict the articles based on user intrests which is given in the form of prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 702
        },
        "id": "il3k3YyAUKPt",
        "outputId": "e7d0516e-5e00-4655-8d37-197a55739d33"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\vukya\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
            "c:\\Users\\vukya\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
            "c:\\Users\\vukya\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.23-gcc_10_3_0.dll\n",
            "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "column_names = ['Item Title', 'Publication Title', 'Item DOI', 'Authors', 'Publication Year', 'URL', 'Keywords', 'Language']\n",
        "\n",
        "main = pd.read_csv('output.csv',encoding=\"utf-8\",header=None, names=column_names)\n",
        "main=main.drop(['Language','Item DOI'],axis=1)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3Fl-nftdxdj"
      },
      "source": [
        "### Journal Dataframe"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Journal articles, keywords, authors concating them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "rAqy-9tSd1LQ",
        "outputId": "715359b6-fcc8-4c48-ca43-4837c57ddb94"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Publication Title</th>\n",
              "      <th>Keywords</th>\n",
              "      <th>Articles</th>\n",
              "      <th>Authors</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AAPPS Bulletin</td>\n",
              "      <td>AAPPS Bulletin Atomic Molecular Optical and Pl...</td>\n",
              "      <td>[Correction to Strong field physics pursued wi...</td>\n",
              "      <td>[Vishwa Bandhu PathakSeong Ku LeeKi Hong PaeCa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>AAPS PharmSciTech</td>\n",
              "      <td>AAPS PharmSciTech PharmacologyToxicology Biote...</td>\n",
              "      <td>[Correction Environmental Monitoring for Close...</td>\n",
              "      <td>[Joseph McCallNonita BarnardKevin GadientChand...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>AI  SOCIETY</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[Correction to Dismantling the Chinese Room wi...</td>\n",
              "      <td>[Lawrence Lengbeyer, Clare L E Foster, Frances...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>AI and Ethics</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[Publisher Correction Convergence of the sourc...</td>\n",
              "      <td>[Haleh Asgarinia, Jakob MkanderLuciano Floridi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>AI in Civil Engineering</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[AI in Civil Engineering, Fusion of thermal an...</td>\n",
              "      <td>[Xianzhong Zhao, Quincy G AlexanderVedhus Hosk...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Publication Title                                           Keywords  \\\n",
              "0           AAPPS Bulletin  AAPPS Bulletin Atomic Molecular Optical and Pl...   \n",
              "1        AAPS PharmSciTech  AAPS PharmSciTech PharmacologyToxicology Biote...   \n",
              "2              AI  SOCIETY                                                NaN   \n",
              "3            AI and Ethics                                                NaN   \n",
              "4  AI in Civil Engineering                                                NaN   \n",
              "\n",
              "                                            Articles  \\\n",
              "0  [Correction to Strong field physics pursued wi...   \n",
              "1  [Correction Environmental Monitoring for Close...   \n",
              "2  [Correction to Dismantling the Chinese Room wi...   \n",
              "3  [Publisher Correction Convergence of the sourc...   \n",
              "4  [AI in Civil Engineering, Fusion of thermal an...   \n",
              "\n",
              "                                             Authors  \n",
              "0  [Vishwa Bandhu PathakSeong Ku LeeKi Hong PaeCa...  \n",
              "1  [Joseph McCallNonita BarnardKevin GadientChand...  \n",
              "2  [Lawrence Lengbeyer, Clare L E Foster, Frances...  \n",
              "3  [Haleh Asgarinia, Jakob MkanderLuciano Floridi...  \n",
              "4  [Xianzhong Zhao, Quincy G AlexanderVedhus Hosk...  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "journal_art = main.groupby('Publication Title')['Item Title'].apply(list).reset_index(name='Articles')\n",
        "journal_art.set_index(['Publication Title'],inplace=True)\n",
        "journal_auth = main.groupby('Publication Title')['Authors'].apply(list).reset_index(name='Authors')\n",
        "journal_auth.set_index(['Publication Title'],inplace=True)\n",
        "journal_key= main.drop_duplicates(subset=[\"Publication Title\", \"Keywords\"], keep='first')\n",
        "journal_key=journal_key.drop(['Item Title','Authors','Publication Year','URL'],axis=1)\n",
        "journal_key.set_index(['Publication Title'],inplace=True)\n",
        "journal_main = pd.concat([journal_key, journal_art,journal_auth], axis=1, join='inner')\n",
        "journal_main=journal_main.reset_index()\n",
        "journal_main.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Changing into paragraphs from lists"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### get paragraph:\n",
        "1. combines joins the list of the data into a paragraph\n",
        "##### get cleaned_text:\n",
        "1. check if the instance is string\n",
        "2. re.match(r'^[a-zA-Z]+$', word) checks if the word consists only of alphabetic characters\n",
        "3. word not in stop_words checks if the word is not in a list of stop words.\n",
        "4. len(word) > 1 checks if the word has a length greater than 1.\n",
        "5. word[1] != '.' checks if the second character of the word is not a dot (.) character.\n",
        "##### combine text:\n",
        "1. combines all the indices required in the list of indices into a paragraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\vukya\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\vukya\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\vukya\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "def get_paragraph(row,index):\n",
        "  ans=''\n",
        "  for x in row[index]:\n",
        "    ans=ans+'  '+x.lower()\n",
        "  return ans\n",
        "def get_clean_text(row,index):\n",
        "    if not isinstance(row[index], str):\n",
        "        return '' \n",
        "    clean_text=''\n",
        "    words = re.findall(r'\\b\\w+\\b', row[index].lower())\n",
        "    words = row[index].lower().split()\n",
        "    for word in words:\n",
        "        if( re.match(r'^[a-zA-Z]+$', word) and  word not in stop_words and len(word) >1 and word[1] != '.'):\n",
        "            clean_text=clean_text+' '+word\n",
        "    return clean_text\n",
        "def combine(row,indices):\n",
        "    ans=''\n",
        "    for i in indices:\n",
        "      ans=ans+' '+row[i]\n",
        "    return ans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Publication Title</th>\n",
              "      <th>Keywords</th>\n",
              "      <th>Articles</th>\n",
              "      <th>Authors</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AAPPS Bulletin</td>\n",
              "      <td>aapps bulletin atomic molecular optical plasm...</td>\n",
              "      <td>correction strong field physics pursued petaw...</td>\n",
              "      <td>vishwa bandhu pathakseong ku leeki hong paeca...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>AAPS PharmSciTech</td>\n",
              "      <td>aaps pharmscitech pharmacologytoxicology biot...</td>\n",
              "      <td>correction environmental monitoring closed ro...</td>\n",
              "      <td>joseph mccallnonita barnardkevin gadientchand...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>AI  SOCIETY</td>\n",
              "      <td></td>\n",
              "      <td>correction dismantling chinese room linguisti...</td>\n",
              "      <td>lawrence lengbeyer clare foster francesca fof...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>AI and Ethics</td>\n",
              "      <td></td>\n",
              "      <td>publisher correction convergence source contr...</td>\n",
              "      <td>haleh asgarinia jakob mkanderluciano floridi ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>AI in Civil Engineering</td>\n",
              "      <td></td>\n",
              "      <td>ai civil engineering fusion thermal rgb image...</td>\n",
              "      <td>xianzhong zhao quincy alexandervedhus hoskere...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Publication Title                                           Keywords  \\\n",
              "0           AAPPS Bulletin   aapps bulletin atomic molecular optical plasm...   \n",
              "1        AAPS PharmSciTech   aaps pharmscitech pharmacologytoxicology biot...   \n",
              "2              AI  SOCIETY                                                      \n",
              "3            AI and Ethics                                                      \n",
              "4  AI in Civil Engineering                                                      \n",
              "\n",
              "                                            Articles  \\\n",
              "0   correction strong field physics pursued petaw...   \n",
              "1   correction environmental monitoring closed ro...   \n",
              "2   correction dismantling chinese room linguisti...   \n",
              "3   publisher correction convergence source contr...   \n",
              "4   ai civil engineering fusion thermal rgb image...   \n",
              "\n",
              "                                             Authors  \n",
              "0   vishwa bandhu pathakseong ku leeki hong paeca...  \n",
              "1   joseph mccallnonita barnardkevin gadientchand...  \n",
              "2   lawrence lengbeyer clare foster francesca fof...  \n",
              "3   haleh asgarinia jakob mkanderluciano floridi ...  \n",
              "4   xianzhong zhao quincy alexandervedhus hoskere...  "
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "journal_main['Articles']=journal_main.apply(get_paragraph,index='Articles',axis=1)\n",
        "journal_main['Articles']=journal_main.apply(get_clean_text,index='Articles',axis=1)\n",
        "journal_main['Authors']=journal_main.apply(get_paragraph,index='Authors',axis=1)\n",
        "journal_main['Authors']=journal_main.apply(get_clean_text,index='Authors',axis=1)\n",
        "journal_main['Keywords']=journal_main.apply(get_clean_text,index=1,axis=1)\n",
        "journal_main.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature Extraction taking nouns and adjectives as features"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Preprocessing the text"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is possible to predict journals using only nouns and adjectives as features. By focusing on nouns and adjectives, we can capture key information and characteristics of the articles that may be relevant for predicting the corresponding journals."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "journal_main['Tokenized'] = journal_main['Articles'].apply(word_tokenize)\n",
        "journal_main['Tagged'] = journal_main['Tokenized'].apply(pos_tag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Publication Title</th>\n",
              "      <th>Tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AAPPS Bulletin</td>\n",
              "      <td>aapps bulletin atomic molecular optical plasm...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>AAPS PharmSciTech</td>\n",
              "      <td>aaps pharmscitech pharmacologytoxicology biot...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>AI  SOCIETY</td>\n",
              "      <td>correction chinese room linguistic tools conc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>AI and Ethics</td>\n",
              "      <td>publisher correction convergence source contr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>AI in Civil Engineering</td>\n",
              "      <td>civil engineering fusion thermal rgb images d...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Publication Title                                               Tags\n",
              "0           AAPPS Bulletin   aapps bulletin atomic molecular optical plasm...\n",
              "1        AAPS PharmSciTech   aaps pharmscitech pharmacologytoxicology biot...\n",
              "2              AI  SOCIETY   correction chinese room linguistic tools conc...\n",
              "3            AI and Ethics   publisher correction convergence source contr...\n",
              "4  AI in Civil Engineering   civil engineering fusion thermal rgb images d..."
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "journal_main['Tags'] = journal_main['Tagged'].apply(lambda x: [word for word, tag in x if tag.startswith('NN') or tag.startswith('JJ')])\n",
        "journal_main['Tags'] = journal_main['Tags'].apply(lambda x: [word for word in x if word.lower() not in stop_words])\n",
        "journal_main['Tags']=journal_main.apply(get_paragraph,index='Tags',axis=1)\n",
        "journal_main=journal_main.drop(['Articles','Tokenized','Tagged'],axis=1)\n",
        "journal_main['Tags']=journal_main.apply(combine,indices=['Keywords','Tags','Authors'],axis=1)\n",
        "journal_main['Tags']=journal_main.apply(get_clean_text,index='Tags',axis=1)\n",
        "journal_main=journal_main.drop(['Keywords','Authors'],axis=1)\n",
        "journal_main.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TFIDF vector for journal prediction "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ZkbEdmV4dsOz"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "vectorizer = TfidfVectorizer(decode_error='ignore',strip_accents='ascii')\n",
        "journal_tfidf_matrix = vectorizer.fit_transform(journal_main['Tags'])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "we predict based on cosine similarities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "journal_threshold=4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDF4ynyZftUU",
        "outputId": "43afa80d-eb18-4b69-f311-31e4fb25cadb"
      },
      "outputs": [],
      "source": [
        "def get_journal_index(user_input):\n",
        "    user_tfidf = vectorizer.transform([user_input])\n",
        "    cosine_similarities = cosine_similarity(user_tfidf, journal_tfidf_matrix).flatten()\n",
        "    indices = cosine_similarities.argsort()[::-1]\n",
        "    top_recommendations = [i for i in indices if cosine_similarities[i] > 0][:min(journal_threshold, len(indices))]\n",
        "    return top_recommendations"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Preparing individual article dataframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "mOvtQlE2ion5"
      },
      "outputs": [],
      "source": [
        "def get_article_df(row):\n",
        "    article = main.loc[main['Publication Title'] == journal_main['Publication Title'][row.name]].copy()\n",
        "    article['Item Title']=article.apply(get_clean_text,index='Item Title',axis=1)\n",
        "    article['Authors']=article.apply(get_clean_text,index='Authors',axis=1)\n",
        "    article['Tokenized'] = article['Item Title'].apply(word_tokenize)\n",
        "    article['Tagged'] = article['Tokenized'].apply(pos_tag)\n",
        "    article['Tags'] = article['Tagged'].apply(lambda x: [word for word, tag in x if tag.startswith('NN') or tag.startswith('JJ')and word.lower() not in stop_words])\n",
        "    article['Tags']=article.apply(get_paragraph,index='Tags',axis=1)\n",
        "    article['Tags']=article.apply(lambda x : x['Tags']+' '+x['Authors']+' '+str(x['Publication Year']),axis=1)\n",
        "    article=article.drop(['Keywords','Publication Title','Tokenized','Tagged','Authors','Publication Year'],axis=1)\n",
        "    article.reset_index(inplace=True)\n",
        "    article.set_index('index', inplace=True)\n",
        "    return article\n",
        "\n",
        "journal_main['article_df']=journal_main.apply(get_article_df,axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjoblib\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpairwise\u001b[39;00m \u001b[39mimport\u001b[39;00m cosine_similarity\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature_extraction\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtext\u001b[39;00m \u001b[39mimport\u001b[39;00m TfidfVectorizer\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdjango\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mshortcuts\u001b[39;00m \u001b[39mimport\u001b[39;00m get_object_or_404\n",
            "File \u001b[1;32mc:\\Users\\vukya\\anaconda3\\lib\\site-packages\\sklearn\\__init__.py:82\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _distributor_init  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m __check_build  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[1;32m---> 82\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m clone\n\u001b[0;32m     83\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_show_versions\u001b[39;00m \u001b[39mimport\u001b[39;00m show_versions\n\u001b[0;32m     85\u001b[0m __all__ \u001b[39m=\u001b[39m [\n\u001b[0;32m     86\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcalibration\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     87\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcluster\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mshow_versions\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    129\u001b[0m ]\n",
            "File \u001b[1;32mc:\\Users\\vukya\\anaconda3\\lib\\site-packages\\sklearn\\base.py:17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m __version__\n\u001b[0;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_config\u001b[39;00m \u001b[39mimport\u001b[39;00m get_config\n\u001b[1;32m---> 17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m _IS_32BIT\n\u001b[0;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_set_output\u001b[39;00m \u001b[39mimport\u001b[39;00m _SetOutputMixin\n\u001b[0;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_tags\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     20\u001b[0m     _DEFAULT_TAGS,\n\u001b[0;32m     21\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\vukya\\anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py:25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdeprecation\u001b[39;00m \u001b[39mimport\u001b[39;00m deprecated\n\u001b[0;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdiscovery\u001b[39;00m \u001b[39mimport\u001b[39;00m all_estimators\n\u001b[1;32m---> 25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mfixes\u001b[39;00m \u001b[39mimport\u001b[39;00m parse_version, threadpool_info\n\u001b[0;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_estimator_html_repr\u001b[39;00m \u001b[39mimport\u001b[39;00m estimator_html_repr\n\u001b[0;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mvalidation\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     28\u001b[0m     as_float_array,\n\u001b[0;32m     29\u001b[0m     assert_all_finite,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     38\u001b[0m     _is_arraylike_not_scalar,\n\u001b[0;32m     39\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\vukya\\anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py:19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstats\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mthreadpoolctl\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdeprecation\u001b[39;00m \u001b[39mimport\u001b[39;00m deprecated\n",
            "File \u001b[1;32mc:\\Users\\vukya\\anaconda3\\lib\\site-packages\\scipy\\stats\\__init__.py:485\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m.. _statsrefmanual:\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    480\u001b[0m \n\u001b[0;32m    481\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    483\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_warnings_errors\u001b[39;00m \u001b[39mimport\u001b[39;00m (ConstantInputWarning, NearConstantInputWarning,\n\u001b[0;32m    484\u001b[0m                                DegenerateDataWarning, FitError)\n\u001b[1;32m--> 485\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_stats_py\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m    486\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_variation\u001b[39;00m \u001b[39mimport\u001b[39;00m variation\n\u001b[0;32m    487\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdistributions\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\vukya\\anaconda3\\lib\\site-packages\\scipy\\stats\\_stats_py.py:46\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mspecial\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mspecial\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m \u001b[39mimport\u001b[39;00m linalg\n\u001b[1;32m---> 46\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m distributions\n\u001b[0;32m     47\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _mstats_basic \u001b[39mas\u001b[39;00m mstats_basic\n\u001b[0;32m     48\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_stats_mstats_common\u001b[39;00m \u001b[39mimport\u001b[39;00m (_find_repeats, linregress, theilslopes,\n\u001b[0;32m     49\u001b[0m                                    siegelslopes)\n",
            "File \u001b[1;32mc:\\Users\\vukya\\anaconda3\\lib\\site-packages\\scipy\\stats\\distributions.py:14\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _discrete_distns\n\u001b[0;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_continuous_distns\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m---> 14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_levy_stable\u001b[39;00m \u001b[39mimport\u001b[39;00m levy_stable\n\u001b[0;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_discrete_distns\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_entropy\u001b[39;00m \u001b[39mimport\u001b[39;00m entropy\n",
            "File \u001b[1;32mc:\\Users\\vukya\\anaconda3\\lib\\site-packages\\scipy\\stats\\_levy_stable\\__init__.py:18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_distn_infrastructure\u001b[39;00m \u001b[39mimport\u001b[39;00m rv_continuous, _ShapeInfo\n\u001b[0;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_continuous_distns\u001b[39;00m \u001b[39mimport\u001b[39;00m uniform, expon, _norm_pdf, _norm_cdf\n\u001b[1;32m---> 18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mlevyst\u001b[39;00m \u001b[39mimport\u001b[39;00m Nolan\n\u001b[0;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_lib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdoccer\u001b[39;00m \u001b[39mimport\u001b[39;00m inherit_docstring_from\n\u001b[0;32m     22\u001b[0m __all__ \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mlevy_stable\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mlevy_stable_gen\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mpdf_from_cf_with_fft\u001b[39m\u001b[39m\"\u001b[39m]\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:404\u001b[0m, in \u001b[0;36mparent\u001b[1;34m(self)\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Generating tfidf matrices for the journals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_vectorizer(row):\n",
        "    vectorizer = TfidfVectorizer(decode_error='ignore',strip_accents='ascii')\n",
        "    return vectorizer\n",
        "def get_tfidf_matrix(row):\n",
        "    tfidf_matrix = row['article_vectorizer'].fit_transform(row['article_df']['Tags'])\n",
        "    return tfidf_matrix\n",
        "journal_main['article_vectorizer']=journal_main.apply(get_vectorizer,axis=1)\n",
        "journal_main['article_matrix']=journal_main.apply(get_tfidf_matrix,axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "article_threshold=10"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Article recommendation using user input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_article_recommendations(user_input):\n",
        "    recommended_journals=get_journal_index(user_input)\n",
        "    l=[]\n",
        "    for journal_id in recommended_journals:\n",
        "        user_tfidf=journal_main['article_vectorizer'][journal_id].transform([user_input])\n",
        "        cosine_similarities = cosine_similarity(user_tfidf, journal_main['article_matrix'][journal_id]).flatten()\n",
        "        indices = cosine_similarities.argsort()[::-1]\n",
        "        top_recommendation_articles = [(cosine_similarities[i],i,journal_id) for i in indices if cosine_similarities[i] > 0][:min(article_threshold, len(indices))]\n",
        "        l=l+top_recommendation_articles\n",
        "        print(journal_id)\n",
        "    l.sort(reverse=True)\n",
        "    return l\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(' language test activism', 'http://link.springer.com/article/10.1007/s10993-022-09614-7', 25, 1524), (' attribution autonomy role robotic language acquisition', 'http://link.springer.com/article/10.1007/s00146-020-01114-8', 196, 2), (' language advocacy times securitization neoliberalization network languagerights', 'http://link.springer.com/article/10.1007/s10993-022-09617-4', 31, 1524), (' bias comparison framework abusive language datasets', 'http://link.springer.com/article/10.1007/s43681-021-00081-0', 119, 3), (' auditing large language models threelayered approach', 'http://link.springer.com/article/10.1007/s43681-023-00289-2', 67, 3), (' officiality strategic ambiguity language policy exploring migrant experiences andorra luxembourg', 'http://link.springer.com/article/10.1007/s10993-021-09602-3', 29, 1524), (' identity ai', 'http://link.springer.com/article/10.1007/s44163-022-00038-0', 59, 599), (' evolution language ideological debates english french multilingual humanitarian organisation', 'http://link.springer.com/article/10.1007/s10993-021-09586-0', 30, 1524), (' analysis news sentiments using natural language processing deep learning', 'http://link.springer.com/article/10.1007/s00146-020-01111-x', 19, 2), (' natural language processing analysis applied opentext opinions using distilbert model sentiment categorization', 'http://link.springer.com/article/10.1007/s00146-022-01594-w', 28, 2), (' automated occupation coding hierarchical features datacentric approach classification pretrained language models', 'http://link.springer.com/article/10.1007/s44163-023-00050-y', 22, 599), (' reflections human role ai policy formulations national ai strategies view people', 'http://link.springer.com/article/10.1007/s44163-022-00019-3', 57, 599), (' ai doctor see assessing framing ai news coverage', 'http://link.springer.com/article/10.1007/s00146-021-01145-9', 167, 2), (' perceptions attitudes qatar university students regarding utility arabic english communication education qatar', 'http://link.springer.com/article/10.1007/s10993-021-09590-4', 26, 1524), (' ai ethics framework measuring embodied carbon ai systems', 'http://link.springer.com/article/10.1007/s43681-021-00071-2', 88, 3), (' making ai society ai futures frames german political media discourses', 'http://link.springer.com/article/10.1007/s00146-021-01161-9', 136, 2), (' tech industry hijacking ai ethics research agenda reclaim', 'http://link.springer.com/article/10.1007/s44163-022-00043-3', 8, 599), (' subnational ai policy shaping ai multilevel governance system', 'http://link.springer.com/article/10.1007/s00146-022-01561-5', 254, 2), (' perspectives ai adoption italy role italian ai strategy', 'http://link.springer.com/article/10.1007/s44163-022-00025-5', 45, 599), (' ai social theory', 'http://link.springer.com/article/10.1007/s00146-021-01222-z', 86, 2), (' importance humanizing ai using behavioral lens bridge gaps humans machines', 'http://link.springer.com/article/10.1007/s44163-022-00030-8', 58, 599), (' uselessness ai ethics', 'http://link.springer.com/article/10.1007/s43681-022-00209-w', 115, 3), (' ai public public interest theory shifts discourse ai', 'http://link.springer.com/article/10.1007/s00146-022-01480-5', 155, 2), (' vision practice lack alignment ai strategies energy regulations dutch electricity sector', 'http://link.springer.com/article/10.1007/s44163-022-00040-6', 5, 599), (' speeding keep exploring use ai research process', 'http://link.springer.com/article/10.1007/s00146-021-01259-0', 188, 2), (' social ethical impacts artificial intelligence agriculture mapping agricultural ai literature', 'http://link.springer.com/article/10.1007/s00146-021-01377-9', 274, 2), (' putting ai ethics work tools fit purpose', 'http://link.springer.com/article/10.1007/s43681-021-00084-x', 31, 3), (' ai ethics systemic risks finance', 'http://link.springer.com/article/10.1007/s43681-021-00129-1', 147, 3), (' public sector ai transparency standard uk government seeks lead example', 'http://link.springer.com/article/10.1007/s44163-022-00018-4', 11, 599), (' glitters gold trustworthy ethical ai principles', 'http://link.springer.com/article/10.1007/s43681-022-00232-x', 108, 3), (' language us communitybased anishinaabemowin language planning using teknology', 'http://link.springer.com/article/10.1007/s10993-023-09656-5', 7, 1524), (' ai impacts supply chain performance manufacturing use case study', 'http://link.springer.com/article/10.1007/s44163-023-00061-9', 52, 599), (' rawlsian ai fairness loopholes', 'http://link.springer.com/article/10.1007/s43681-022-00226-9', 13, 3), (' explainable ai lacks regulative reasons ai human decisionmaking equally opaque', 'http://link.springer.com/article/10.1007/s43681-022-00217-w', 45, 3), (' ai recruiting unethical human rights perspective use ai hiring', 'http://link.springer.com/article/10.1007/s43681-022-00166-4', 86, 3), (' language languageineducation planning multilingual india minoritized language perspective', 'http://link.springer.com/article/10.1007/s10993-015-9397-4', 46, 1524), (' charting ai urbanism conceptual sources spatial implications urban artificial intelligence', 'http://link.springer.com/article/10.1007/s44163-023-00060-w', 29, 599), (' normative language policy minority language rights rethinking case regional languages france', 'http://link.springer.com/article/10.1007/s10993-016-9411-5', 40, 1524), (' linguistic imperialism still valid construct relation language policy irish sign language', 'http://link.springer.com/article/10.1007/s10993-017-9446-2', 39, 1524), (' claudine chamoreau isabelle lglise eds dynamics contactinduced language change language contact bilingualism series', 'http://link.springer.com/article/10.1007/s10993-013-9282-y', 37, 1524)]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def get_links(user_input):\n",
        "    l=[]\n",
        "    recommendation_list=get_article_recommendations(user_input) #You will get article id and journal id from here\n",
        "    for article in recommendation_list:\n",
        "        cosine_similarity,article_id,journal_id=article\n",
        "        l.append((journal_main['article_df'][journal_id].iloc[article_id,0],journal_main['article_df'][journal_id].iloc[article_id,1],article_id,journal_id))\n",
        "        # print(name,url)\n",
        "        print()\n",
        "    return l\n",
        "user_input = \"AI is my favourite language in 2022\"\n",
        "l=get_links(user_input)\n",
        "print(l)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['journal_df.joblib']"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import joblib\n",
        "joblib.dump(journal_main,'journal_df.joblib')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['journal_tfidf.joblib']"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "joblib.dump(journal_tfidf_matrix,'journal_tfidf.joblib')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['journal_vectorizer.joblib']"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "joblib.dump(vectorizer,'journal_vectorizer.joblib')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Publication Year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>279624.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>2018.662740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>4.398261</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1971.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2017.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>2020.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>2022.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>2023.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Publication Year\n",
              "count     279624.000000\n",
              "mean        2018.662740\n",
              "std            4.398261\n",
              "min         1971.000000\n",
              "25%         2017.000000\n",
              "50%         2020.000000\n",
              "75%         2022.000000\n",
              "max         2023.000000"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "main.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\vukya\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
            "c:\\Users\\vukya\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
            "c:\\Users\\vukya\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.23-gcc_10_3_0.dll\n",
            "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
          ]
        }
      ],
      "source": [
        "import joblib\n",
        "import os\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "BASE_DIR=os.getcwd()\n",
        "JOURNAL_VECTORIZER = joblib.load('../joblib/vectorizer.joblib')\n",
        "JOURNAL_TFIDF_MATRIX = joblib.load('../joblib/journal_tfidf.joblib')\n",
        "\n",
        "journal_threshold = 4\n",
        "\n",
        "\n",
        "\n",
        "def get_journal_index(user_input):\n",
        "    user_tfidf = JOURNAL_VECTORIZER.transform([user_input])\n",
        "    cosine_similarities = cosine_similarity(user_tfidf, JOURNAL_TFIDF_MATRIX).flatten()\n",
        "    indices = cosine_similarities.argsort()[::-1]\n",
        "    top_recommendations = [i for i in indices if cosine_similarities[i] > 0][:min(journal_threshold, len(indices))]\n",
        "    print(top_recommendations)\n",
        "    return top_recommendations\n",
        "    \n",
        "article_threshold = 10\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_article_recommendations(user_input):\n",
        "    l=[]\n",
        "    recommended_journals = get_journal_index(user_input)\n",
        "    for journal_id in recommended_journals:\n",
        "        tfidf_matrix_file_path=os.path.join(f\"../joblib/journal_tfidf_{journal_id}.joblib\")\n",
        "        vectorizer_file_path=os.path.join(f\"../joblib/vectorizer_{journal_id}.joblib\")\n",
        "        tfidf_matrix=joblib.load(tfidf_matrix_file_path)\n",
        "        vectorizer=joblib.load(vectorizer_file_path)\n",
        "        user_tfidf = vectorizer.transform([user_input])\n",
        "        cosine_similarities = cosine_similarity(user_tfidf, tfidf_matrix).flatten()\n",
        "        indices = cosine_similarities.argsort()[::-1]\n",
        "        print(cosine_similarities)\n",
        "        top_recommendation_articles = [(cosine_similarities[i], i, journal_id) for i in indices if cosine_similarities[i] > 0][:min(5, len(indices))]\n",
        "        l.extend(top_recommendation_articles)\n",
        "    l.sort(reverse=True)\n",
        "    return l\n",
        "\n",
        "def get_links(user_input):\n",
        "    l = []\n",
        "    print(\"hi from get_links\")\n",
        "    recommendation_list = get_article_recommendations(user_input)\n",
        "    # print(recommendation_list)\n",
        "    for article in recommendation_list:\n",
        "        cosine_similarity, article_id, journal_id = article\n",
        "        l.append(article)\n",
        "    return l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import joblib\n",
        "import os\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_matrix_file_path=os.path.join(f\"../joblib/journal_tfidf_{100}.joblib\")\n",
        "vectorizer_file_path=os.path.join(f\"../joblib/vectorizer_{100}.joblib\")\n",
        "tfidf_matrix=joblib.load(tfidf_matrix_file_path)\n",
        "vectorizer=joblib.load(vectorizer_file_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['abstract', 'allairemariapia', 'analysis', 'behavior',\n",
              "       'convergence', 'denson', 'diffractive', 'diractype', 'equation',\n",
              "       'equations', 'exponents', 'fritz', 'functions', 'gasiskinikolaos',\n",
              "       'gesztesyjerome', 'goldbergsachs', 'goldsteinhelge', 'goverc',\n",
              "       'green', 'grgoire', 'hillpawe', 'hlder', 'holdengerald',\n",
              "       'infinity', 'julia', 'kosek', 'leszek', 'marta', 'media',\n",
              "       'nurowski', 'operators', 'palombarojeffrey', 'papageorgiou',\n",
              "       'periodic', 'polynomial', 'problems', 'rauch', 'rod', 'sets',\n",
              "       'sharp', 'teschl', 'version', 'weak'], dtype=object)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vectorizer.get_feature_names_out()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hi from get_links\n",
            "[758, 2036, 2035, 169]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.18321437 0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.21001715 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.31299354\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.        ]\n",
            "[(0.31299354385050054, 359, 169), (0.21001715357073258, 133, 169), (0.1832143662335961, 22, 169)]\n"
          ]
        }
      ],
      "source": [
        "user_input=\"sex\"\n",
        "print(get_links(user_input))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
